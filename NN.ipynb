{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL - Erkennung\n",
    "\n",
    "## Intro\n",
    "\n",
    "Stellen sie sich vor, sie wollen sich mit einem Taubstummen unterhalten, aber beherschen die Zeichensprache nicht. Unsere Applikation bietet ihnen eine real-time Lösung, die mit einer Bilderkennung Handzeichen in Buchstaben übersetzt. \n",
    "\n",
    "## Architektur unseres Neuronales Netzwerks\n",
    "\n",
    "Die Basis unseres neuronalem Netzwerks bildet das Modell VGG16, das mit 14 Millionen Bildern vortrainiert ist und Features extrahieren kann. Diese Features nutzt unser eigens erstellter Klassifizierer um ein Eingabebild einem Buchstaben zuzuordnen. \n",
    "ü ö ä\n",
    "## Entwicklungsprozess\n",
    "\n",
    "Angefangen haben damit die Bilder des Trainingsdatensatzes zu importieren und mit den richtigen Labels zu versehen und ein erstes Netzwerk zu trainieren. Dabei haben wir uns anfangs nur auf \"A\", \"B\" und \"Nichts\" beschränkt um das Problem klein zu halten und mögliche Probleme frühzeitig zu erkennen. Daraufhin haben wir das Netzwerk exportiert damit wir es in unserer Applikation als Prediction Machine nutzen können. Mit pygame Modul haben wir dann das Interface gebaut und die Webcam als live Input implementiert. Als alles funktionierte konnten wir zum ersten Mal unser Programm in der echten Welt testen und wir bemerkten schnell, dass die korrekte Klassifizierung niedrig war. Aufgrund dessen experimentierten wir mit Hauttonerkennung um Hintergrundstörungen zu entfernen. Doch es stellte sich schnell heraus, dass aufgrund der Schatten, die bei manchen Handzeichen entstehen, die Erkennung nur mittelmäßig funktionierte und das Ergebnis verschlechterte. Dafür erweiterten wir den Datensatz mit eigens aufgenommen Daten und verbesserten somit das Ergebnis erheblich. Dann nahmen wir immmer mehr Buchstaben in unser Netzwerk auf. Erst sechs, dann zwölf, dann 21 und letzendlich 29, aber je mehr Klassen hinzu kamen desto ungenauer wurde unser Ergebnis. \n",
    "Unser Plan ist, durch Ausnutzung \\\\ mehrerer Bilder in einem kleinen Zeitintervall die Vorhersage zu verbessern.\n",
    "\n",
    "## Reflektion\n",
    "\n",
    "Leider funktioniert unser NN letzendlich nicht so gut wie wir uns es anfangs ausgemalt haben. Solange wir uns nur auf wenige Zeichen beschränken funktioniert unsere Applikation genau so wie wir uns es auch vorgestellt haben. Bei fast 30 Klassen wird es aber unumgänglich sein deutlich mehr Trainingsdaten hinzuzufügen. Ein komplexeres Netzwerk (reinforcement learning) oder Fine Tuning des ganzen Modells um das Erebnis leicht zu verbessern. \n",
    "\n",
    "## Ausblick\n",
    "\n",
    "Sobald das Programm auf dem Computer sehr zuverlässig funktioniert, planen wir eine mobile App zu bauen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "{'1': 'A', '2': 'B', '3': 'C', '4': 'D', '5': 'E', '6': 'F', '7': 'G', '8': 'H', '9': 'I', '10': 'J', '11': 'K', '12': 'L', '13': 'M', '14': 'N', '15': 'O', '16': 'P', '17': 'Q', '18': 'R', '27': 'del', '28': 'nothing', '29': 'space'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('jsons/middle21.json', 'r') as f:\n",
    "    flower_to_name = json.load(f)\n",
    "    \n",
    "print(len(flower_to_name)) \n",
    "print(flower_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/leonardo/Documents/jupyter/ASL/data-batch'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/valid'\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
    "                                          transforms.RandomResizedCrop(224),\n",
    "                                          transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                               [0.229, 0.224, 0.225])])\n",
    "\n",
    "validation_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                            transforms.CenterCrop(224),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                                 [0.229, 0.224, 0.225])])\n",
    "\n",
    "testing_transforms = transforms.Compose([transforms.Resize(256),\n",
    "                                         transforms.CenterCrop(224),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                              [0.229, 0.224, 0.225])])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "training_dataset = datasets.ImageFolder(train_dir, transform=training_transforms)\n",
    "validation_dataset = datasets.ImageFolder(valid_dir, transform=validation_transforms)\n",
    "testing_dataset = datasets.ImageFolder(test_dir, transform=testing_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32)\n",
    "test_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained model parameters\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 8000)),\n",
    "                                        ('relu', nn.ReLU()),\n",
    "                                        ('drop', nn.Dropout(p=0.5)),\n",
    "                                        ('fc2', nn.Linear(8000, 21)),\n",
    "                                        ('output', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, validateloader, criterion):\n",
    "    \n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    \n",
    "    for images, labels in iter(validateloader):\n",
    "\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        output = model.forward(images)\n",
    "        val_loss += criterion(output, labels).item()\n",
    "\n",
    "        probabilities = torch.exp(output)\n",
    "        \n",
    "        equality = (labels.data == probabilities.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "    \n",
    "    return val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and gradient descent\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3..  Training Loss: 6.181..  Validation Loss: 2.305..  Validation Accuracy: 0.300\n",
      "Epoch: 1/3..  Training Loss: 2.356..  Validation Loss: 1.684..  Validation Accuracy: 0.456\n",
      "Epoch: 1/3..  Training Loss: 2.104..  Validation Loss: 1.257..  Validation Accuracy: 0.581\n",
      "Epoch: 1/3..  Training Loss: 1.971..  Validation Loss: 1.344..  Validation Accuracy: 0.565\n",
      "Epoch: 1/3..  Training Loss: 1.857..  Validation Loss: 0.990..  Validation Accuracy: 0.685\n",
      "Epoch: 1/3..  Training Loss: 1.781..  Validation Loss: 1.084..  Validation Accuracy: 0.636\n",
      "Epoch: 1/3..  Training Loss: 1.733..  Validation Loss: 1.056..  Validation Accuracy: 0.651\n",
      "Epoch: 1/3..  Training Loss: 1.648..  Validation Loss: 0.802..  Validation Accuracy: 0.718\n",
      "Epoch: 1/3..  Training Loss: 1.632..  Validation Loss: 0.795..  Validation Accuracy: 0.739\n",
      "Epoch: 1/3..  Training Loss: 1.599..  Validation Loss: 0.726..  Validation Accuracy: 0.735\n",
      "Epoch: 1/3..  Training Loss: 1.571..  Validation Loss: 0.802..  Validation Accuracy: 0.727\n",
      "Epoch: 1/3..  Training Loss: 1.573..  Validation Loss: 0.827..  Validation Accuracy: 0.714\n",
      "Epoch: 1/3..  Training Loss: 1.510..  Validation Loss: 0.927..  Validation Accuracy: 0.702\n",
      "Epoch: 1/3..  Training Loss: 1.529..  Validation Loss: 0.689..  Validation Accuracy: 0.762\n",
      "Epoch: 1/3..  Training Loss: 1.448..  Validation Loss: 0.818..  Validation Accuracy: 0.731\n",
      "Epoch: 1/3..  Training Loss: 1.499..  Validation Loss: 0.808..  Validation Accuracy: 0.731\n",
      "Epoch: 1/3..  Training Loss: 1.451..  Validation Loss: 0.753..  Validation Accuracy: 0.744\n",
      "Epoch: 1/3..  Training Loss: 1.471..  Validation Loss: 0.620..  Validation Accuracy: 0.781\n",
      "Epoch: 1/3..  Training Loss: 1.423..  Validation Loss: 0.643..  Validation Accuracy: 0.776\n",
      "Epoch: 1/3..  Training Loss: 1.431..  Validation Loss: 0.659..  Validation Accuracy: 0.770\n",
      "Epoch: 1/3..  Training Loss: 1.388..  Validation Loss: 0.628..  Validation Accuracy: 0.779\n",
      "Epoch: 1/3..  Training Loss: 1.361..  Validation Loss: 0.713..  Validation Accuracy: 0.758\n",
      "Epoch: 1/3..  Training Loss: 1.393..  Validation Loss: 0.602..  Validation Accuracy: 0.791\n",
      "Epoch: 1/3..  Training Loss: 1.405..  Validation Loss: 0.594..  Validation Accuracy: 0.783\n",
      "Epoch: 1/3..  Training Loss: 1.368..  Validation Loss: 0.619..  Validation Accuracy: 0.793\n",
      "Epoch: 1/3..  Training Loss: 1.364..  Validation Loss: 0.638..  Validation Accuracy: 0.783\n",
      "Epoch: 1/3..  Training Loss: 1.324..  Validation Loss: 0.575..  Validation Accuracy: 0.791\n",
      "Epoch: 1/3..  Training Loss: 1.324..  Validation Loss: 0.520..  Validation Accuracy: 0.816\n",
      "Epoch: 1/3..  Training Loss: 1.317..  Validation Loss: 0.492..  Validation Accuracy: 0.824\n",
      "Epoch: 1/3..  Training Loss: 1.296..  Validation Loss: 0.553..  Validation Accuracy: 0.797\n",
      "Epoch: 1/3..  Training Loss: 1.331..  Validation Loss: 0.605..  Validation Accuracy: 0.784\n",
      "Epoch: 1/3..  Training Loss: 1.353..  Validation Loss: 0.550..  Validation Accuracy: 0.812\n",
      "Epoch: 1/3..  Training Loss: 1.284..  Validation Loss: 0.514..  Validation Accuracy: 0.814\n",
      "Epoch: 1/3..  Training Loss: 1.297..  Validation Loss: 0.564..  Validation Accuracy: 0.802\n",
      "Epoch: 1/3..  Training Loss: 1.251..  Validation Loss: 0.464..  Validation Accuracy: 0.829\n",
      "Epoch: 1/3..  Training Loss: 1.273..  Validation Loss: 0.604..  Validation Accuracy: 0.787\n",
      "Epoch: 1/3..  Training Loss: 1.249..  Validation Loss: 0.519..  Validation Accuracy: 0.818\n",
      "Epoch: 1/3..  Training Loss: 1.283..  Validation Loss: 0.588..  Validation Accuracy: 0.804\n",
      "Epoch: 1/3..  Training Loss: 1.279..  Validation Loss: 0.602..  Validation Accuracy: 0.791\n",
      "Epoch: 1/3..  Training Loss: 1.294..  Validation Loss: 0.663..  Validation Accuracy: 0.788\n",
      "Epoch: 1/3..  Training Loss: 1.270..  Validation Loss: 0.635..  Validation Accuracy: 0.786\n",
      "Epoch: 1/3..  Training Loss: 1.189..  Validation Loss: 0.549..  Validation Accuracy: 0.817\n",
      "Epoch: 1/3..  Training Loss: 1.210..  Validation Loss: 0.533..  Validation Accuracy: 0.810\n",
      "Epoch: 2/3..  Training Loss: 1.093..  Validation Loss: 0.639..  Validation Accuracy: 0.786\n",
      "Epoch: 2/3..  Training Loss: 1.248..  Validation Loss: 0.694..  Validation Accuracy: 0.777\n",
      "Epoch: 2/3..  Training Loss: 1.290..  Validation Loss: 0.639..  Validation Accuracy: 0.785\n",
      "Epoch: 2/3..  Training Loss: 1.284..  Validation Loss: 0.568..  Validation Accuracy: 0.803\n",
      "Epoch: 2/3..  Training Loss: 1.276..  Validation Loss: 0.604..  Validation Accuracy: 0.790\n",
      "Epoch: 2/3..  Training Loss: 1.221..  Validation Loss: 0.541..  Validation Accuracy: 0.801\n",
      "Epoch: 2/3..  Training Loss: 1.270..  Validation Loss: 0.556..  Validation Accuracy: 0.806\n",
      "Epoch: 2/3..  Training Loss: 1.297..  Validation Loss: 0.480..  Validation Accuracy: 0.836\n",
      "Epoch: 2/3..  Training Loss: 1.190..  Validation Loss: 0.476..  Validation Accuracy: 0.838\n",
      "Epoch: 2/3..  Training Loss: 1.201..  Validation Loss: 0.460..  Validation Accuracy: 0.836\n",
      "Epoch: 2/3..  Training Loss: 1.217..  Validation Loss: 0.522..  Validation Accuracy: 0.821\n",
      "Epoch: 2/3..  Training Loss: 1.184..  Validation Loss: 0.624..  Validation Accuracy: 0.791\n",
      "Epoch: 2/3..  Training Loss: 1.164..  Validation Loss: 0.529..  Validation Accuracy: 0.831\n",
      "Epoch: 2/3..  Training Loss: 1.213..  Validation Loss: 0.476..  Validation Accuracy: 0.830\n",
      "Epoch: 2/3..  Training Loss: 1.188..  Validation Loss: 0.450..  Validation Accuracy: 0.845\n",
      "Epoch: 2/3..  Training Loss: 1.205..  Validation Loss: 0.552..  Validation Accuracy: 0.827\n",
      "Epoch: 2/3..  Training Loss: 1.150..  Validation Loss: 0.481..  Validation Accuracy: 0.829\n",
      "Epoch: 2/3..  Training Loss: 1.132..  Validation Loss: 0.532..  Validation Accuracy: 0.823\n",
      "Epoch: 2/3..  Training Loss: 1.150..  Validation Loss: 0.634..  Validation Accuracy: 0.792\n",
      "Epoch: 2/3..  Training Loss: 1.248..  Validation Loss: 0.598..  Validation Accuracy: 0.801\n",
      "Epoch: 2/3..  Training Loss: 1.222..  Validation Loss: 0.588..  Validation Accuracy: 0.808\n",
      "Epoch: 2/3..  Training Loss: 1.145..  Validation Loss: 0.620..  Validation Accuracy: 0.813\n",
      "Epoch: 2/3..  Training Loss: 1.169..  Validation Loss: 0.571..  Validation Accuracy: 0.813\n",
      "Epoch: 2/3..  Training Loss: 1.184..  Validation Loss: 0.530..  Validation Accuracy: 0.823\n",
      "Epoch: 2/3..  Training Loss: 1.161..  Validation Loss: 0.493..  Validation Accuracy: 0.829\n",
      "Epoch: 2/3..  Training Loss: 1.169..  Validation Loss: 0.587..  Validation Accuracy: 0.806\n",
      "Epoch: 2/3..  Training Loss: 1.153..  Validation Loss: 0.602..  Validation Accuracy: 0.818\n",
      "Epoch: 2/3..  Training Loss: 1.162..  Validation Loss: 0.591..  Validation Accuracy: 0.819\n",
      "Epoch: 2/3..  Training Loss: 1.141..  Validation Loss: 0.698..  Validation Accuracy: 0.790\n",
      "Epoch: 2/3..  Training Loss: 1.156..  Validation Loss: 0.542..  Validation Accuracy: 0.825\n",
      "Epoch: 2/3..  Training Loss: 1.120..  Validation Loss: 0.636..  Validation Accuracy: 0.812\n",
      "Epoch: 2/3..  Training Loss: 1.211..  Validation Loss: 0.596..  Validation Accuracy: 0.816\n",
      "Epoch: 2/3..  Training Loss: 1.124..  Validation Loss: 0.641..  Validation Accuracy: 0.811\n",
      "Epoch: 2/3..  Training Loss: 1.111..  Validation Loss: 0.480..  Validation Accuracy: 0.837\n",
      "Epoch: 2/3..  Training Loss: 1.184..  Validation Loss: 0.525..  Validation Accuracy: 0.827\n",
      "Epoch: 2/3..  Training Loss: 1.139..  Validation Loss: 0.554..  Validation Accuracy: 0.819\n",
      "Epoch: 2/3..  Training Loss: 1.107..  Validation Loss: 0.561..  Validation Accuracy: 0.822\n",
      "Epoch: 2/3..  Training Loss: 1.235..  Validation Loss: 0.499..  Validation Accuracy: 0.835\n",
      "Epoch: 2/3..  Training Loss: 1.122..  Validation Loss: 0.507..  Validation Accuracy: 0.831\n",
      "Epoch: 2/3..  Training Loss: 1.157..  Validation Loss: 0.526..  Validation Accuracy: 0.816\n",
      "Epoch: 2/3..  Training Loss: 1.141..  Validation Loss: 0.527..  Validation Accuracy: 0.825\n",
      "Epoch: 2/3..  Training Loss: 1.120..  Validation Loss: 0.546..  Validation Accuracy: 0.835\n",
      "Epoch: 2/3..  Training Loss: 1.122..  Validation Loss: 0.534..  Validation Accuracy: 0.829\n",
      "Epoch: 3/3..  Training Loss: 0.849..  Validation Loss: 0.502..  Validation Accuracy: 0.829\n",
      "Epoch: 3/3..  Training Loss: 1.101..  Validation Loss: 0.562..  Validation Accuracy: 0.815\n",
      "Epoch: 3/3..  Training Loss: 1.148..  Validation Loss: 0.515..  Validation Accuracy: 0.839\n",
      "Epoch: 3/3..  Training Loss: 1.085..  Validation Loss: 0.481..  Validation Accuracy: 0.843\n",
      "Epoch: 3/3..  Training Loss: 1.107..  Validation Loss: 0.477..  Validation Accuracy: 0.844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3/3..  Training Loss: 1.107..  Validation Loss: 0.553..  Validation Accuracy: 0.832\n",
      "Epoch: 3/3..  Training Loss: 1.119..  Validation Loss: 0.610..  Validation Accuracy: 0.824\n",
      "Epoch: 3/3..  Training Loss: 1.140..  Validation Loss: 0.605..  Validation Accuracy: 0.824\n",
      "Epoch: 3/3..  Training Loss: 1.139..  Validation Loss: 0.591..  Validation Accuracy: 0.820\n",
      "Epoch: 3/3..  Training Loss: 1.096..  Validation Loss: 0.576..  Validation Accuracy: 0.822\n",
      "Epoch: 3/3..  Training Loss: 1.133..  Validation Loss: 0.551..  Validation Accuracy: 0.827\n",
      "Epoch: 3/3..  Training Loss: 1.093..  Validation Loss: 0.534..  Validation Accuracy: 0.826\n",
      "Epoch: 3/3..  Training Loss: 1.122..  Validation Loss: 0.503..  Validation Accuracy: 0.834\n",
      "Epoch: 3/3..  Training Loss: 1.077..  Validation Loss: 0.537..  Validation Accuracy: 0.838\n",
      "Epoch: 3/3..  Training Loss: 1.128..  Validation Loss: 0.505..  Validation Accuracy: 0.831\n",
      "Epoch: 3/3..  Training Loss: 1.104..  Validation Loss: 0.447..  Validation Accuracy: 0.851\n",
      "Epoch: 3/3..  Training Loss: 1.116..  Validation Loss: 0.469..  Validation Accuracy: 0.838\n",
      "Epoch: 3/3..  Training Loss: 1.101..  Validation Loss: 0.565..  Validation Accuracy: 0.833\n",
      "Epoch: 3/3..  Training Loss: 1.124..  Validation Loss: 0.593..  Validation Accuracy: 0.818\n",
      "Epoch: 3/3..  Training Loss: 1.128..  Validation Loss: 0.534..  Validation Accuracy: 0.829\n",
      "Epoch: 3/3..  Training Loss: 1.116..  Validation Loss: 0.497..  Validation Accuracy: 0.836\n",
      "Epoch: 3/3..  Training Loss: 1.124..  Validation Loss: 0.533..  Validation Accuracy: 0.824\n",
      "Epoch: 3/3..  Training Loss: 1.117..  Validation Loss: 0.547..  Validation Accuracy: 0.819\n",
      "Epoch: 3/3..  Training Loss: 1.120..  Validation Loss: 0.494..  Validation Accuracy: 0.839\n",
      "Epoch: 3/3..  Training Loss: 1.089..  Validation Loss: 0.504..  Validation Accuracy: 0.834\n",
      "Epoch: 3/3..  Training Loss: 1.057..  Validation Loss: 0.570..  Validation Accuracy: 0.834\n",
      "Epoch: 3/3..  Training Loss: 1.094..  Validation Loss: 0.489..  Validation Accuracy: 0.852\n",
      "Epoch: 3/3..  Training Loss: 1.079..  Validation Loss: 0.495..  Validation Accuracy: 0.839\n",
      "Epoch: 3/3..  Training Loss: 1.088..  Validation Loss: 0.502..  Validation Accuracy: 0.841\n",
      "Epoch: 3/3..  Training Loss: 1.076..  Validation Loss: 0.553..  Validation Accuracy: 0.830\n",
      "Epoch: 3/3..  Training Loss: 1.061..  Validation Loss: 0.531..  Validation Accuracy: 0.844\n",
      "Epoch: 3/3..  Training Loss: 1.086..  Validation Loss: 0.466..  Validation Accuracy: 0.844\n",
      "Epoch: 3/3..  Training Loss: 1.077..  Validation Loss: 0.534..  Validation Accuracy: 0.826\n",
      "Epoch: 3/3..  Training Loss: 1.056..  Validation Loss: 0.518..  Validation Accuracy: 0.829\n",
      "Epoch: 3/3..  Training Loss: 1.090..  Validation Loss: 0.595..  Validation Accuracy: 0.826\n",
      "Epoch: 3/3..  Training Loss: 1.033..  Validation Loss: 0.496..  Validation Accuracy: 0.836\n",
      "Epoch: 3/3..  Training Loss: 1.135..  Validation Loss: 0.546..  Validation Accuracy: 0.832\n",
      "Epoch: 3/3..  Training Loss: 1.036..  Validation Loss: 0.692..  Validation Accuracy: 0.818\n",
      "Epoch: 3/3..  Training Loss: 1.076..  Validation Loss: 0.698..  Validation Accuracy: 0.816\n",
      "Epoch: 3/3..  Training Loss: 1.066..  Validation Loss: 0.614..  Validation Accuracy: 0.824\n",
      "Epoch: 3/3..  Training Loss: 1.082..  Validation Loss: 0.586..  Validation Accuracy: 0.833\n",
      "Epoch: 3/3..  Training Loss: 1.100..  Validation Loss: 0.602..  Validation Accuracy: 0.842\n",
      "Epoch: 3/3..  Training Loss: 1.089..  Validation Loss: 0.700..  Validation Accuracy: 0.828\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "\n",
    "def train_classifier():\n",
    "\n",
    "\n",
    "    epochs = 3\n",
    "    steps = 0\n",
    "    print_every = 40\n",
    "\n",
    "    model.to('cuda')\n",
    "\n",
    "    for e in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in iter(train_loader):\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps % print_every == 0:\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                # Turn off gradients for validation, saves memory and computations\n",
    "                with torch.no_grad():\n",
    "                    validation_loss, accuracy = validation(model, validate_loader, criterion)\n",
    "\n",
    "                print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                      \"Training Loss: {:.3f}.. \".format(running_loss/print_every),\n",
    "                      \"Validation Loss: {:.3f}.. \".format(validation_loss/len(validate_loader)),\n",
    "                      \"Validation Accuracy: {:.3f}\".format(accuracy/len(validate_loader)))\n",
    "\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "                    \n",
    "train_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(model, test_loader):\n",
    "\n",
    "    # Do validation on the test set\n",
    "    model.eval()\n",
    "    model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        accuracy = 0\n",
    "    \n",
    "        for images, labels in iter(test_loader):\n",
    "    \n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            output = model.forward(images)\n",
    "\n",
    "            probabilities = torch.exp(output)\n",
    "        \n",
    "            equality = (labels.data == probabilities.max(dim=1)[1])\n",
    "        \n",
    "            accuracy += equality.type(torch.FloatTensor).mean()\n",
    "        \n",
    "        print(\"Test Accuracy: {}\".format(accuracy/len(test_loader)))    \n",
    "        \n",
    "        \n",
    "test_accuracy(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the checkpoint\n",
    "\n",
    "def save_checkpoint(model):\n",
    "\n",
    "    model.class_to_idx = training_dataset.class_to_idx\n",
    "\n",
    "    checkpoint = {'arch': \"vgg16\",\n",
    "                  'class_to_idx': model.class_to_idx,\n",
    "                  'model_state_dict': model.state_dict()\n",
    "                 }\n",
    "\n",
    "    torch.save(checkpoint, 'NNs/ABCDEFGHIJKLMNOPQRdns3Gextra1.pth')\n",
    "    \n",
    "save_checkpoint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Function that loads a checkpoint and rebuilds the model\n",
    "\n",
    "def load_checkpoint(filepath):\n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    \n",
    "    if checkpoint['arch'] == 'vgg16':\n",
    "        \n",
    "        model = models.vgg16(pretrained=True)\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        print(\"Architecture not recognized.\")\n",
    "    \n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25088, 5000)),\n",
    "                                            ('relu', nn.ReLU()),\n",
    "                                            ('drop', nn.Dropout(p=0.5)),\n",
    "                                            ('fc2', nn.Linear(5000, 3)),\n",
    "                                            ('output', nn.LogSoftmax(dim=1))]))\n",
    "\n",
    "    model.classifier = classifier\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_checkpoint('NNs/ABn3G.pth')\n",
    "model.cuda()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def process_image(image_path):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # Process a PIL image for use in a PyTorch model\n",
    "    \n",
    "    pil_image = Image.open(image_path)\n",
    "    \n",
    "    # Resize\n",
    "    if pil_image.size[0] > pil_image.size[1]:\n",
    "        pil_image.thumbnail((5000, 256))\n",
    "    else:\n",
    "        pil_image.thumbnail((256, 5000))\n",
    "        \n",
    "    # Crop \n",
    "    left_margin = (pil_image.width-224)/2\n",
    "    bottom_margin = (pil_image.height-224)/2\n",
    "    right_margin = left_margin + 224\n",
    "    top_margin = bottom_margin + 224\n",
    "    \n",
    "    pil_image = pil_image.crop((left_margin, bottom_margin, right_margin, top_margin))\n",
    "    \n",
    "    # Normalize\n",
    "    np_image = np.array(pil_image)/255\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    np_image = (np_image - mean) / std\n",
    "    \n",
    "    # PyTorch expects the color channel to be the first dimension but it's the third dimension in the PIL image and Numpy array\n",
    "    # Color channel needs to be first; retain the order of the other two dimensions.\n",
    "    np_image = np_image.transpose((2, 0, 1))\n",
    "    \n",
    "    return np_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # PyTorch tensors assume the color channel is the first dimension\n",
    "    # but matplotlib assumes is the third dimension\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    \n",
    "    # Undo preprocessing\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "image = process_image('A_test.jpg')\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=2):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    image = process_image(image_path)\n",
    "    \n",
    "    # Convert image to PyTorch tensor first\n",
    "    image = torch.from_numpy(image).type(torch.cuda.FloatTensor)\n",
    "    #print(image.shape)\n",
    "    #print(type(image))\n",
    "    \n",
    "    # Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    output = model.forward(image)\n",
    "    \n",
    "    probabilities = torch.exp(output)\n",
    "    \n",
    "    # Probabilities and the indices of those probabilities corresponding to the classes\n",
    "    top_probabilities, top_indices = probabilities.topk(topk)\n",
    "    \n",
    "    # Convert to lists\n",
    "    top_probabilities = top_probabilities.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
    "    top_indices = top_indices.detach().type(torch.FloatTensor).numpy().tolist()[0] \n",
    "    \n",
    "    # Convert topk_indices to the actual class labels using class_to_idx\n",
    "    # Invert the dictionary so you get a mapping from index to class.\n",
    "    \n",
    "    idx_to_class = {value: key for key, value in model.class_to_idx.items()}\n",
    "    #print(idx_to_class)\n",
    "    \n",
    "    top_classes = [idx_to_class[index] for index in top_indices]\n",
    "    \n",
    "    return top_probabilities, top_classes\n",
    "    \n",
    "probs, classes = predict('A_test.jpg', model)   \n",
    "print(probs)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display an image along with the top 5 classes\n",
    "\n",
    "# Plot flower input image\n",
    "plt.figure(figsize = (6,10))\n",
    "plot_1 = plt.subplot(2,1,1)\n",
    "\n",
    "image = process_image('A_test.jpg')\n",
    "\n",
    "flower_title = flower_to_name['1']\n",
    "\n",
    "imshow(image, plot_1, title=flower_title);\n",
    "\n",
    "# Convert from the class integer encoding to actual flower names\n",
    "flower_names = [flower_to_name[i] for i in classes]\n",
    "\n",
    "# Plot the probabilities for the top 5 classes as a bar graph\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "sb.barplot(x=probs, y=flower_names, color=sb.color_palette()[0]);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
